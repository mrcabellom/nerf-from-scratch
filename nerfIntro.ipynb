{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tuqdtdTR4yQk"
      },
      "source": [
        "# Introducción a NERF (Neural Radiance Fields)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Durante los últimos años, han aparecido bastante investigaciones acerca de la creación de escenas 3D a partir de imágenes 2D. NERF o Neural Radience Fields es una técnica bastante en el mundo de Deep Learning y Computer Vision. El primer paper que se publicó al respecto fue en el 2020: [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, Mildenhall, Srinivasan, Tancik et al. (2020).](https://arxiv.org/abs/2003.08934)\n",
        "\n",
        "A lo largo de este notebook explicaremos en detalle cada uno de los conceptos que componen esta novedosa técnica. Además, implementaremos cada uno de los módulos que conforman esta nueva técnica. Al final este notebook seremos capaces de entrenar con Pytorch nuestro propio modelo NERF. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arquitectura NERF"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los NERF se basan en un concepto ya conocido que son los campos de luz o campos de radiación. Un campo de luz es una función que describe como se produce el transporte de luz a lo largo del un volumen 3D. Se Describe la dirección de los rayos de luz que se mueven a través de cada coordenada x=(x, y, z) en el espacio y en cada dirección d, descrita como ángulos θ y ϕ o un vector unitario. En conjunto, forman un espacio de características 5D."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Input](docs/architecture.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " El modelo NeRF que se inspira en la representación anteriormente descrita intenta aproximar una función que mapea desde este espacio a un espacio 4D que consta de color c=(R,G,B) y una densidad σ, que se puede considerar como la probabilidad de que la luz rayo en este espacio de coordenadas 5D se termina (por ejemplo, por oclusión). El NeRF estándar es, por tanto, una función de la forma F : (x,d) -> (c,σ)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v2fo27VhBlnr"
      },
      "source": [
        "## Pipeline\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Pipeline](docs/pipeline.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importación de Librerías\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Optional, Tuple, List, Union, Callable\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "from tqdm import trange\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "toIYVxPL5IDO"
      },
      "source": [
        "## Conjunto de datos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNLc9hbd6JGK"
      },
      "source": [
        "Este conjunto de datos consta de 106 imágenes tomadas de la excavadora sintética Lego junto con poses y un valor de distancia focal común. Al igual que el paper original, reservamos las primeras 100 imágenes para entrenamiento y una sola imagen de prueba para validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "csCQJhSzhnEW",
        "outputId": "72c19550-e6a3-4a44-f108-e8b7ac61c599"
      },
      "outputs": [],
      "source": [
        "data = np.load('data/tiny_nerf_data.npz')\n",
        "images = data['images']\n",
        "poses = data['poses']\n",
        "focal = data['focal']\n",
        "\n",
        "print(f'Images shape: {images.shape}')\n",
        "print(f'Poses shape: {poses.shape}')\n",
        "print(f'Focal length: {focal}')\n",
        "\n",
        "height, width = images.shape[1:3]\n",
        "near, far = 2., 6.\n",
        "\n",
        "n_training = 100\n",
        "testimg_idx = 101\n",
        "testimg, testpose = images[testimg_idx], poses[testimg_idx]\n",
        "\n",
        "plt.imshow(testimg)\n",
        "print('Pose')\n",
        "print(testpose)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa8rlY5djYvt"
      },
      "source": [
        "## Orígenes y direcciones\n",
        "\n",
        "Para muestrear puntos del espacio 3D más allá de nuestra imagen, primero comenzamos desde la pose inicial de cada cámara tomada en el conjunto de fotos. Haciendo uso de operaciones vectoriales, podemos convertir estas matrices de pose 4x4 en una coordenada 3D que indica el origen y un vector 3D que indica la dirección. Estos valores juntos, describen un vector que indica hacia dónde apuntaba una cámara cuando se tomó la foto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Get the origins and directions from camera poses\n",
        "origins = poses[:, :3, -1]\n",
        "dirs = np.stack([np.sum([0, 0, -1] * pose[:3, :3], axis=-1) for pose in poses])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Wvq5Y663jYHE",
        "outputId": "0b268313-421a-4ca7-b3ee-a13406cb6f73"
      },
      "outputs": [],
      "source": [
        "ax = plt.figure(figsize=(12, 8)).add_subplot(projection='3d')\n",
        "_ = ax.quiver(\n",
        "  origins[..., 0].flatten(),\n",
        "  origins[..., 1].flatten(),\n",
        "  origins[..., 2].flatten(),\n",
        "  dirs[..., 0].flatten(),\n",
        "  dirs[..., 1].flatten(),\n",
        "  dirs[..., 2].flatten(), length=0.5, normalize=True)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('z')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vUI5BA9BXlTm"
      },
      "source": [
        "Con esta pose de cámara, ahora podemos encontrar las líneas de proyección a lo largo de cada píxel de nuestra imagen. Cada línea está definida por su punto de origen (x,y,z) y su dirección (en este caso, un vector 3D). Si bien el origen es el mismo para cada píxel, la dirección es ligeramente diferente. Estas líneas están ligeramente desviadas del centro, de modo que ninguna de estas líneas es paralela.\n",
        "\n",
        "![Pinhole camera](https://www.researchgate.net/profile/Willy-Azarcoya-Cabiedes/publication/317498100/figure/fig10/AS:610418494013440@1522546518034/Pin-hole-camera-model-terminology-The-optical-center-pinhole-is-placed-at-the-origin.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHNwlsOT7NTp"
      },
      "outputs": [],
      "source": [
        "def get_rays(\n",
        "  height: int,\n",
        "  width: int,\n",
        "  focal_length: float,\n",
        "  camera_pose: torch.Tensor\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Find origin and direction of rays through every pixel and camera origin.\n",
        "  \"\"\"\n",
        "\n",
        "  # Apply pinhole camera model to gather directions at each pixel\n",
        "  i, j = torch.meshgrid(\n",
        "      torch.arange(width, dtype=torch.float32).to(camera_pose),\n",
        "      torch.arange(height, dtype=torch.float32).to(camera_pose),\n",
        "      indexing='ij')\n",
        "  i, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
        "  directions = torch.stack([(i - width * .5) / focal_length,\n",
        "                            -(j - height * .5) / focal_length,\n",
        "                            -torch.ones_like(i)\n",
        "                           ], dim=-1)\n",
        "\n",
        "  # Apply camera pose to directions\n",
        "  rays_d = torch.sum(directions[..., None, :] * camera_pose[:3, :3], dim=-1)\n",
        "\n",
        "  # Origin is same for all directions (the optical center)\n",
        "  rays_o = camera_pose[:3, -1].expand(rays_d.shape)\n",
        "  return rays_o, rays_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYPFqClGkKD3",
        "outputId": "135c2dea-858a-4d98-84bd-5b74df7075ac"
      },
      "outputs": [],
      "source": [
        "testpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aivi7gXLkPTP",
        "outputId": "668bfa0b-702d-4766-8ecb-fedc7cdd19f1"
      },
      "outputs": [],
      "source": [
        "focal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoV1r440lCQB",
        "outputId": "2e8f6bd2-cbdb-41c6-dfab-0b37e13b260b"
      },
      "outputs": [],
      "source": [
        "\n",
        "images = torch.from_numpy(data['images'][:n_training]).to(device)\n",
        "poses = torch.from_numpy(data['poses']).to(device)\n",
        "focal = torch.from_numpy(data['focal']).to(device)\n",
        "testimg = torch.from_numpy(data['images'][testimg_idx]).to(device)\n",
        "testpose = torch.from_numpy(data['poses'][testimg_idx]).to(device)\n",
        "\n",
        "height, width = images.shape[1:3]\n",
        "with torch.no_grad():\n",
        "  ray_origin, ray_direction = get_rays(height, width, focal, testpose)\n",
        "\n",
        "print('Ray Origin')\n",
        "print(ray_origin.shape)\n",
        "print(ray_origin[height // 2, width // 2, :])\n",
        "print('')\n",
        "\n",
        "print('Ray Direction')\n",
        "print(ray_direction.shape)\n",
        "print(ray_direction[height // 2, width // 2, :])\n",
        "print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ray_direction_np = ray_direction.cpu().numpy()\n",
        "ray_origin_np = ray_origin.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = np.stack((ray_origin_np,ray_direction_np), axis=2)\n",
        "result = result.reshape(-1,2,3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pintar todos los rayos a través de cada pixel (Pinhole)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ax = plt.figure(figsize=(6, 6)).add_subplot(projection='3d')\n",
        "\n",
        "_ = ax.quiver(\n",
        "  result[..., 0, 0],\n",
        "  result[..., 0, 1],\n",
        "  result[..., 0, 2],\n",
        "  result[..., 1, 0],\n",
        "  result[..., 1, 1],\n",
        "  result[..., 1, 2], length=20, normalize=True)\n",
        "\n",
        "# for i in range(0,result.shape[0]):\n",
        "#     ax.plot(result[i,:,0],result[i,:,1],result[i,:,2],c=\"blue\")\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('z')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EU1QDn66CQob"
      },
      "source": [
        "# Arquitectura"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yg3f_12FYjw"
      },
      "source": [
        "## Muestreo estratificado\n",
        "\n",
        "Ahora que tenemos estas líneas, definidas como vectores de origen y dirección se puede comenzar con el proceso de la obtención de muestras. NeRF adopta una estrategia de muestreo de grueso a fino, comenzando con el enfoque de muestreo estratificado.\n",
        "\n",
        "El enfoque de muestreo estratificado divide el rayo en contenedores espaciados uniformemente y toma muestras al azar dentro de cada uno de ellos. La configuración de \"perturbación\" determina si se muestrean puntos uniformemente de cada contenedor o simplemente se usa el centro del contenedor como punto. En la mayoría de los casos, es recomendable añadir una pertubación para evitar el overfiting de la red y así ayudar a la red a aprender sobre un espacio de muestreo continuo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAAfDK2L-faR"
      },
      "outputs": [],
      "source": [
        "def sample_stratified(\n",
        "  rays_o: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  near: float,\n",
        "  far: float,\n",
        "  n_samples: int,\n",
        "  perturb: Optional[bool] = True,\n",
        "  inverse_depth: bool = False\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Sample along ray from regularly-spaced bins.\n",
        "  \"\"\"\n",
        "\n",
        "  # Grab samples for space integration along ray\n",
        "  t_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
        "  if not inverse_depth:\n",
        "    # Sample linearly between `near` and `far`\n",
        "    z_vals = near * (1.-t_vals) + far * (t_vals)\n",
        "  else:\n",
        "    # Sample linearly in inverse depth (disparity)\n",
        "    z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
        "\n",
        "  # Draw uniform samples from bins along ray\n",
        "  if perturb:\n",
        "    mids = .5 * (z_vals[1:] + z_vals[:-1])\n",
        "    upper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
        "    lower = torch.concat([z_vals[:1], mids], dim=-1)\n",
        "    t_rand = torch.rand([n_samples], device=z_vals.device)\n",
        "    z_vals = lower + (upper - lower) * t_rand\n",
        "  z_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
        "\n",
        "  # Apply scale from `rays_d` and offset from `rays_o` to samples\n",
        "  # pts: (width, height, n_samples, 3)\n",
        "  pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
        "  return pts, z_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCWWyvuLm0VE",
        "outputId": "71e97362-a91c-45ba-e9ce-f8acc8fb59e8"
      },
      "outputs": [],
      "source": [
        "# Draw stratified samples from example\n",
        "rays_o = ray_origin.view([-1, 3])\n",
        "rays_d = ray_direction.view([-1, 3])\n",
        "n_samples = 8\n",
        "inverse_depth = False\n",
        "with torch.no_grad():\n",
        "  pts, z_vals = sample_stratified(rays_o, rays_d, near, far, n_samples,\n",
        "                                  perturb=True, inverse_depth=inverse_depth)\n",
        "\n",
        "print('Input Points')\n",
        "print(pts.shape)\n",
        "print('')\n",
        "print('Distances Along Ray')\n",
        "print(z_vals.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wEeLXT1pc-c2"
      },
      "source": [
        "Ahora visualizamos estos puntos muestreados. Los puntos azules imperturbados son los \"centros\" de los contenedores. Los puntos rojos son una muestra de puntos perturbados. Observe cómo los puntos rojos están ligeramente desplazados de los puntos azules sobre ellos, pero todos están restringidos entre \"cerca\" y \"lejos\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "lt3zLUginJ0B",
        "outputId": "481a7f55-218f-465f-8b1c-b00d353b28a2"
      },
      "outputs": [],
      "source": [
        "y_vals = torch.zeros_like(z_vals)\n",
        "\n",
        "_, z_vals_unperturbed = sample_stratified(rays_o, rays_d, near, far, n_samples,\n",
        "                                  perturb=False, inverse_depth=inverse_depth)\n",
        "plt.plot(z_vals_unperturbed[0].cpu().numpy(), 1 + y_vals[0].cpu().numpy(), 'b-o')\n",
        "plt.plot(z_vals[0].cpu().numpy(), y_vals[0].cpu().numpy(), 'r-o')\n",
        "plt.ylim([-1, 2])\n",
        "plt.title('Stratified Sampling (blue) with Perturbation (red)')\n",
        "ax = plt.gca()\n",
        "ax.axes.yaxis.set_visible(False)\n",
        "plt.grid(True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc0cS74xFb-G"
      },
      "source": [
        "## Positional Encoder\n",
        "\n",
        "Al igual que los transformers, los NeRF utilizan Positional encoders. En este caso, es para mapear las entradas a un espacio de mayor frecuencia para compensar el sesgo que tienen las redes neuronales para aprender funciones de menor frecuencia.\n",
        "\n",
        "Aquí construimos un `torch.nn.Module` simple de nuestro codificador posicional. La misma implementación del codificador se puede aplicar tanto a las muestras de entrada como a las direcciones de visualización. Sin embargo, elegimos diferentes parámetros para estas entradas. Usamos la configuración predeterminada del original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrbs7YoMHAbF"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "  r\"\"\"\n",
        "  Sine-cosine positional encoder for input points.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    d_input: int,\n",
        "    n_freqs: int,\n",
        "    log_space: bool = False\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.d_input = d_input\n",
        "    self.n_freqs = n_freqs\n",
        "    self.log_space = log_space\n",
        "    self.d_output = d_input * (1 + 2 * self.n_freqs)\n",
        "    self.embed_fns = [lambda x: x]\n",
        "\n",
        "    # Define frequencies in either linear or log scale\n",
        "    if self.log_space:\n",
        "      freq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
        "    else:\n",
        "      freq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
        "\n",
        "    # Alternate sin and cos\n",
        "    for freq in freq_bands:\n",
        "      self.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
        "      self.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
        "  \n",
        "  def forward(\n",
        "    self,\n",
        "    x\n",
        "  ) -> torch.Tensor:\n",
        "    r\"\"\"\n",
        "    Apply positional encoding to input.\n",
        "    \"\"\"\n",
        "    return torch.concat([fn(x) for fn in self.embed_fns], dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG9LrSLQvH46",
        "outputId": "a5244374-2e15-4dee-a215-23896bb1621a"
      },
      "outputs": [],
      "source": [
        "# Create encoders for points and view directions\n",
        "encoder = PositionalEncoder(3, 10)\n",
        "viewdirs_encoder = PositionalEncoder(3, 4)\n",
        "\n",
        "# Grab flattened points and view directions\n",
        "pts_flattened = pts.reshape(-1, 3)\n",
        "viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "flattened_viewdirs = viewdirs[:, None, ...].expand(pts.shape).reshape((-1, 3))\n",
        "\n",
        "# Encode inputs\n",
        "encoded_points = encoder(pts_flattened)\n",
        "encoded_viewdirs = viewdirs_encoder(flattened_viewdirs)\n",
        "\n",
        "print('Encoded Points')\n",
        "print(encoded_points.shape)\n",
        "print(torch.min(encoded_points), torch.max(encoded_points), torch.mean(encoded_points))\n",
        "print('')\n",
        "\n",
        "print(encoded_viewdirs.shape)\n",
        "print('Encoded Viewdirs')\n",
        "print(torch.min(encoded_viewdirs), torch.max(encoded_viewdirs), torch.mean(encoded_viewdirs))\n",
        "print('')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "94jKMMz7gb7Y"
      },
      "source": [
        "## NeRF Model\n",
        "\n",
        "Aquí definimos el modelo NeRF, que consiste principalmente en una 'ModuleList' de capas 'Linear', separadas por funciones de activación no lineales y la conexión residual ocasional. Este modelo presenta una entrada opcional para ver las direcciones, que alterará la arquitectura del modelo si se proporciona en la creación de instancias. Esta implementación se basa en la Sección 3 del paper \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\" y utiliza los mismos valores predeterminados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvIDtf3TgaOA"
      },
      "outputs": [],
      "source": [
        "class NeRF(nn.Module):\n",
        "  r\"\"\"\n",
        "  Neural radiance fields module.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    d_input: int = 3,\n",
        "    n_layers: int = 8,\n",
        "    d_filter: int = 256,\n",
        "    skip: Tuple[int] = (4,),\n",
        "    d_viewdirs: Optional[int] = None\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.d_input = d_input\n",
        "    self.skip = skip\n",
        "    self.act = nn.functional.relu\n",
        "    self.d_viewdirs = d_viewdirs\n",
        "\n",
        "    # Create model layers\n",
        "    self.layers = nn.ModuleList(\n",
        "      [nn.Linear(self.d_input, d_filter)] +\n",
        "      [nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
        "       else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
        "    )\n",
        "\n",
        "    # Bottleneck layers\n",
        "    if self.d_viewdirs is not None:\n",
        "      # If using viewdirs, split alpha and RGB\n",
        "      self.alpha_out = nn.Linear(d_filter, 1)\n",
        "      self.rgb_filters = nn.Linear(d_filter, d_filter)\n",
        "      self.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
        "      self.output = nn.Linear(d_filter // 2, 3)\n",
        "    else:\n",
        "      # If no viewdirs, use simpler output\n",
        "      self.output = nn.Linear(d_filter, 4)\n",
        "  \n",
        "  def forward(\n",
        "    self,\n",
        "    x: torch.Tensor,\n",
        "    viewdirs: Optional[torch.Tensor] = None\n",
        "  ) -> torch.Tensor:\n",
        "    r\"\"\"\n",
        "    Forward pass with optional view direction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Cannot use viewdirs if instantiated with d_viewdirs = None\n",
        "    if self.d_viewdirs is None and viewdirs is not None:\n",
        "      raise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
        "\n",
        "    # Apply forward pass up to bottleneck\n",
        "    x_input = x\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      x = self.act(layer(x))\n",
        "      if i in self.skip:\n",
        "        x = torch.cat([x, x_input], dim=-1)\n",
        "\n",
        "    # Apply bottleneck\n",
        "    if self.d_viewdirs is not None:\n",
        "      # Split alpha from network output\n",
        "      alpha = self.alpha_out(x)\n",
        "\n",
        "      # Pass through bottleneck to get RGB\n",
        "      x = self.rgb_filters(x)\n",
        "      x = torch.concat([x, viewdirs], dim=-1)\n",
        "      x = self.act(self.branch(x))\n",
        "      x = self.output(x)\n",
        "\n",
        "      # Concatenate alphas to output\n",
        "      x = torch.concat([x, alpha], dim=-1)\n",
        "    else:\n",
        "      # Simple output\n",
        "      x = self.output(x)\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xckd4RuFitJ"
      },
      "source": [
        "## Volume Rendering\n",
        "\n",
        "A partir de las salidas de NeRF sin procesar, todavía tenemos que convertirlas en una imagen. Aquí es donde aplicamos la integración de volumen descrita en las Ecuaciones 1-3 en la Sección 4 del paper. Básicamente, tomamos la suma ponderada de todas las muestras a lo largo del rayo de cada píxel para obtener el valor de color estimado en ese píxel. Cada muestra RGB se pondera por su valor alfa. Los valores alfa más altos indican una mayor probabilidad de que el área muestreada sea opaca, por lo tanto, es más probable que los puntos más alejados del rayo estén ocluidos. El producto acumulativo asegura que esos puntos adicionales se amortigüen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8Dvz6DszOUT"
      },
      "outputs": [],
      "source": [
        "def cumprod_exclusive(\n",
        "  tensor: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "\n",
        "  # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
        "  cumprod = torch.cumprod(tensor, -1)\n",
        "  # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
        "  cumprod = torch.roll(cumprod, 1, -1)\n",
        "  # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
        "  cumprod[..., 0] = 1.\n",
        "  \n",
        "  return cumprod\n",
        "\n",
        "def raw2outputs(\n",
        "  raw: torch.Tensor,\n",
        "  z_vals: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  raw_noise_std: float = 0.0,\n",
        "  white_bkgd: bool = False\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Convert the raw NeRF output into RGB and other maps.\n",
        "  \"\"\"\n",
        "\n",
        "  # Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
        "  dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
        "  dists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
        "\n",
        "  # Multiply each distance by the norm of its corresponding direction ray\n",
        "  # to convert to real world distance (accounts for non-unit directions).\n",
        "  dists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
        "\n",
        "  # Add noise to model's predictions for density. Can be used to \n",
        "  # regularize network during training (prevents floater artifacts).\n",
        "  noise = 0.\n",
        "  if raw_noise_std > 0.:\n",
        "    noise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
        "\n",
        "  # Predict density of each sample along each ray. Higher values imply\n",
        "  # higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
        "  alpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists)\n",
        "\n",
        "  # Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
        "  # The higher the alpha, the lower subsequent weights are driven.\n",
        "  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
        "\n",
        "  # Compute weighted RGB map.\n",
        "  rgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
        "  rgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
        "\n",
        "  # Estimated depth map is predicted distance.\n",
        "  depth_map = torch.sum(weights * z_vals, dim=-1)\n",
        "\n",
        "  # Disparity map is inverse depth.\n",
        "  disp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map),\n",
        "                            depth_map / torch.sum(weights, -1))\n",
        "\n",
        "  # Sum of weights along each ray. In [0, 1] up to numerical error.\n",
        "  acc_map = torch.sum(weights, dim=-1)\n",
        "\n",
        "  # To composite onto a white background, use the accumulated alpha map.\n",
        "  if white_bkgd:\n",
        "    rgb_map = rgb_map + (1. - acc_map[..., None])\n",
        "\n",
        "  return rgb_map, depth_map, acc_map, weights"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ySqZmYMRFoF1"
      },
      "source": [
        "## Hierarchical Volume Sampling\n",
        "\n",
        "De hecho, el espacio 3D es muy escaso con oclusiones, por lo que la mayoría de los puntos no contribuyen mucho a la imagen renderizada. Por lo tanto, es más beneficioso sobremuestrear regiones con una alta probabilidad de contribuir a la integral. Aquí aplicamos ponderaciones normalizadas aprendidas al primer conjunto de muestras para crear un \"Probability Distribution function\" (PDF) a través del rayo, luego aplicamos el muestreo de transformación inversa a este PDF para recopilar un segundo conjunto de muestras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK2x2JEFFnxA"
      },
      "outputs": [],
      "source": [
        "def sample_pdf(\n",
        "  bins: torch.Tensor,\n",
        "  weights: torch.Tensor,\n",
        "  n_samples: int,\n",
        "  perturb: bool = False\n",
        ") -> torch.Tensor:\n",
        "  r\"\"\"\n",
        "  Apply inverse transform sampling to a weighted set of points.\n",
        "  \"\"\"\n",
        "\n",
        "  # Normalize weights to get PDF.\n",
        "  pdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -1, keepdims=True) # [n_rays, weights.shape[-1]]\n",
        "\n",
        "  # Convert PDF to Cumulative distribution Function.\n",
        "  cdf = torch.cumsum(pdf, dim=-1) # [n_rays, weights.shape[-1]]\n",
        "  cdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1) # [n_rays, weights.shape[-1] + 1]\n",
        "\n",
        "  # Take sample positions to grab from CDF. Linear when perturb == 0.\n",
        "  if not perturb:\n",
        "    u = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
        "    u = u.expand(list(cdf.shape[:-1]) + [n_samples]) # [n_rays, n_samples]\n",
        "  else:\n",
        "    u = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device) # [n_rays, n_samples]\n",
        "\n",
        "  # Find indices along CDF where values in u would be placed.\n",
        "  u = u.contiguous() # Returns contiguous tensor with same values.\n",
        "  inds = torch.searchsorted(cdf, u, right=True) # [n_rays, n_samples]\n",
        "\n",
        "  # Clamp indices that are out of bounds.\n",
        "  below = torch.clamp(inds - 1, min=0)\n",
        "  above = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
        "  inds_g = torch.stack([below, above], dim=-1) # [n_rays, n_samples, 2]\n",
        "\n",
        "  # Sample from cdf and the corresponding bin centers.\n",
        "  matched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]]\n",
        "  cdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
        "                       index=inds_g)\n",
        "  bins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
        "                        index=inds_g)\n",
        "\n",
        "  # Convert samples to ray length.\n",
        "  denom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
        "  denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
        "  t = (u - cdf_g[..., 0]) / denom\n",
        "  samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
        "\n",
        "  return samples # [n_rays, n_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU4qRGMhNNHu"
      },
      "outputs": [],
      "source": [
        "def sample_hierarchical(\n",
        "  rays_o: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  z_vals: torch.Tensor,\n",
        "  weights: torch.Tensor,\n",
        "  n_samples: int,\n",
        "  perturb: bool = False\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Apply hierarchical sampling to the rays.\n",
        "  \"\"\"\n",
        "\n",
        "  # Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
        "  z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
        "  new_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples,\n",
        "                          perturb=perturb)\n",
        "  new_z_samples = new_z_samples.detach()\n",
        "\n",
        "  # Resample points from ray based on PDF.\n",
        "  z_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
        "  pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
        "  return pts, z_vals_combined, new_z_samples"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XFUaajNpNNgJ"
      },
      "source": [
        "## Full Forward Pass\n",
        "\n",
        "Aquí es donde juntamos todo para calcular un solo paso hacia adelante a través de nuestro modelo.\n",
        "\n",
        "Debido a posibles problemas de memoria, el paso hacia adelante se calcula en \"fragmentos\", que luego se agregan en un solo lote. La propagación del gradiente se realiza después de procesar todo el lote, de ahí la distinción entre \"chunks\" y \"batches\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9SbAqC6Ki9H"
      },
      "outputs": [],
      "source": [
        "def get_chunks(\n",
        "  inputs: torch.Tensor,\n",
        "  chunksize: int = 2**15\n",
        ") -> List[torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Divide an input into chunks.\n",
        "  \"\"\"\n",
        "  return [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
        "\n",
        "def prepare_chunks(\n",
        "  points: torch.Tensor,\n",
        "  encoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
        "  chunksize: int = 2**15\n",
        ") -> List[torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Encode and chunkify points to prepare for NeRF model.\n",
        "  \"\"\"\n",
        "  points = points.reshape((-1, 3))\n",
        "  points = encoding_function(points)\n",
        "  points = get_chunks(points, chunksize=chunksize)\n",
        "  return points\n",
        "\n",
        "def prepare_viewdirs_chunks(\n",
        "  points: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  encoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
        "  chunksize: int = 2**15\n",
        ") -> List[torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Encode and chunkify viewdirs to prepare for NeRF model.\n",
        "  \"\"\"\n",
        "  # Prepare the viewdirs\n",
        "  viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "  viewdirs = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n",
        "  viewdirs = encoding_function(viewdirs)\n",
        "  viewdirs = get_chunks(viewdirs, chunksize=chunksize)\n",
        "  return viewdirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY2Dt8lgWhKO"
      },
      "outputs": [],
      "source": [
        "def nerf_forward(\n",
        "  rays_o: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  near: float,\n",
        "  far: float,\n",
        "  encoding_fn: Callable[[torch.Tensor], torch.Tensor],\n",
        "  coarse_model: nn.Module,\n",
        "  kwargs_sample_stratified: dict = None,\n",
        "  n_samples_hierarchical: int = 0,\n",
        "  kwargs_sample_hierarchical: dict = None,\n",
        "  fine_model = None,\n",
        "  viewdirs_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
        "  chunksize: int = 2**15\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
        "  r\"\"\"\n",
        "  Compute forward pass through model(s).\n",
        "  \"\"\"\n",
        "\n",
        "  # Set no kwargs if none are given.\n",
        "  if kwargs_sample_stratified is None:\n",
        "    kwargs_sample_stratified = {}\n",
        "  if kwargs_sample_hierarchical is None:\n",
        "    kwargs_sample_hierarchical = {}\n",
        "  \n",
        "  # Sample query points along each ray.\n",
        "  query_points, z_vals = sample_stratified(\n",
        "      rays_o, rays_d, near, far, **kwargs_sample_stratified)\n",
        "\n",
        "  # Prepare batches.\n",
        "  batches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
        "  if viewdirs_encoding_fn is not None:\n",
        "    batches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
        "                                               viewdirs_encoding_fn,\n",
        "                                               chunksize=chunksize)\n",
        "  else:\n",
        "    batches_viewdirs = [None] * len(batches)\n",
        "\n",
        "  # Coarse model pass.\n",
        "  # Split the encoded points into \"chunks\", run the model on all chunks, and\n",
        "  # concatenate the results (to avoid out-of-memory issues).\n",
        "  predictions = []\n",
        "  for batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
        "    predictions.append(coarse_model(batch, viewdirs=batch_viewdirs))\n",
        "  raw = torch.cat(predictions, dim=0)\n",
        "  raw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
        "\n",
        "  # Perform differentiable volume rendering to re-synthesize the RGB image.\n",
        "  rgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals, rays_d)\n",
        "  # rgb_map, depth_map, acc_map, weights = render_volume_density(raw, rays_o, z_vals)\n",
        "  outputs = {\n",
        "      'z_vals_stratified': z_vals\n",
        "  }\n",
        "\n",
        "  # Fine model pass.\n",
        "  if n_samples_hierarchical > 0:\n",
        "    # Save previous outputs to return.\n",
        "    rgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n",
        "\n",
        "    # Apply hierarchical sampling for fine query points.\n",
        "    query_points, z_vals_combined, z_hierarch = sample_hierarchical(\n",
        "      rays_o, rays_d, z_vals, weights, n_samples_hierarchical,\n",
        "      **kwargs_sample_hierarchical)\n",
        "\n",
        "    # Prepare inputs as before.\n",
        "    batches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
        "    if viewdirs_encoding_fn is not None:\n",
        "      batches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
        "                                                 viewdirs_encoding_fn,\n",
        "                                                 chunksize=chunksize)\n",
        "    else:\n",
        "      batches_viewdirs = [None] * len(batches)\n",
        "\n",
        "    # Forward pass new samples through fine model.\n",
        "    fine_model = fine_model if fine_model is not None else coarse_model\n",
        "    predictions = []\n",
        "    for batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
        "      predictions.append(fine_model(batch, viewdirs=batch_viewdirs))\n",
        "    raw = torch.cat(predictions, dim=0)\n",
        "    raw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
        "\n",
        "    # Perform differentiable volume rendering to re-synthesize the RGB image.\n",
        "    rgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals_combined, rays_d)\n",
        "    \n",
        "    # Store outputs.\n",
        "    outputs['z_vals_hierarchical'] = z_hierarch\n",
        "    outputs['rgb_map_0'] = rgb_map_0\n",
        "    outputs['depth_map_0'] = depth_map_0\n",
        "    outputs['acc_map_0'] = acc_map_0\n",
        "\n",
        "  # Store outputs.\n",
        "  outputs['rgb_map'] = rgb_map\n",
        "  outputs['depth_map'] = depth_map\n",
        "  outputs['acc_map'] = acc_map\n",
        "  outputs['weights'] = weights\n",
        "  return outputs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CtpbzoRJYsK-"
      },
      "source": [
        "# Entrenamiento\n",
        "\n",
        "Por fin tenemos (casi) todo lo que necesitamos para entrenar el modelo. Ahora haremos una configuración para un procedimiento de entrenamiento simple, creando hiperparámetros y funciones auxiliares, luego entrenaremos nuestro modelo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iBXWfThMYtkR"
      },
      "source": [
        "## Hiperparámetros\n",
        "\n",
        "Todos los hiperparámetros para el entrenamiento se establecen aquí. Los valores predeterminados se tomaron del original,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JovhcSy1NIhr"
      },
      "outputs": [],
      "source": [
        "# Encoders\n",
        "d_input = 3           # Number of input dimensions\n",
        "n_freqs = 10          # Number of encoding functions for samples\n",
        "log_space = True      # If set, frequencies scale in log space\n",
        "use_viewdirs = True   # If set, use view direction as input\n",
        "n_freqs_views = 4     # Number of encoding functions for views\n",
        "\n",
        "# Stratified sampling\n",
        "n_samples = 64         # Number of spatial samples per ray\n",
        "perturb = True         # If set, applies noise to sample positions\n",
        "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
        "\n",
        "# Model\n",
        "d_filter = 128          # Dimensions of linear layer filters\n",
        "n_layers = 2            # Number of layers in network bottleneck\n",
        "skip = []               # Layers at which to apply input residual\n",
        "use_fine_model = True   # If set, creates a fine model\n",
        "d_filter_fine = 128     # Dimensions of linear layer filters of fine network\n",
        "n_layers_fine = 6       # Number of layers in fine network bottleneck\n",
        "\n",
        "# Hierarchical sampling\n",
        "n_samples_hierarchical = 64   # Number of samples per ray\n",
        "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
        "\n",
        "# Optimizer\n",
        "lr = 5e-4  # Learning rate\n",
        "\n",
        "# Training\n",
        "n_iters = 10000\n",
        "batch_size = 2**14          # Number of rays per gradient step (power of 2)\n",
        "one_image_per_step = True   # One image per gradient step (disables batching)\n",
        "chunksize = 2**14           # Modify as needed to fit in GPU memory\n",
        "center_crop = True          # Crop the center of image (one_image_per_)\n",
        "center_crop_iters = 50      # Stop cropping center after this many epochs\n",
        "display_rate = 25        # Display test output every X epochs\n",
        "\n",
        "# Early Stopping\n",
        "warmup_iters = 100          # Number of iterations during warmup phase\n",
        "warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n",
        "n_restarts = 10             # Number of times to restart if training stalls\n",
        "\n",
        "# We bundle the kwargs for various functions to pass all at once.\n",
        "kwargs_sample_stratified = {\n",
        "    'n_samples': n_samples,\n",
        "    'perturb': perturb,\n",
        "    'inverse_depth': inverse_depth\n",
        "}\n",
        "kwargs_sample_hierarchical = {\n",
        "    'perturb': perturb\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ODAQKoAUY0KJ"
      },
      "source": [
        "## Clases y funciones de entrenamiento\n",
        "\n",
        "Aquí creamos algunas funciones auxiliares para el entrenamiento. NeRF puede ser propenso a mínimos locales, en los que el entrenamiento se estancará rápidamente y producirá resultados en blanco. `EarlyStopping` se utiliza para reiniciar el entrenamiento cuando el aprendizaje se detiene, si es necesario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeIrS4P54piy"
      },
      "outputs": [],
      "source": [
        "def plot_samples(\n",
        "  z_vals: torch.Tensor,\n",
        "  z_hierarch: Optional[torch.Tensor] = None,\n",
        "  ax: Optional[np.ndarray] = None):\n",
        "  r\"\"\"\n",
        "  Plot stratified and (optional) hierarchical samples.\n",
        "  \"\"\"\n",
        "  y_vals = 1 + np.zeros_like(z_vals)\n",
        "\n",
        "  if ax is None:\n",
        "    ax = plt.subplot()\n",
        "  ax.plot(z_vals, y_vals, 'b-o')\n",
        "  if z_hierarch is not None:\n",
        "    y_hierarch = np.zeros_like(z_hierarch)\n",
        "    ax.plot(z_hierarch, y_hierarch, 'r-o')\n",
        "  ax.set_ylim([-1, 2])\n",
        "  ax.set_title('Stratified  Samples (blue) and Hierarchical Samples (red)')\n",
        "  ax.axes.yaxis.set_visible(False)\n",
        "  ax.grid(True)\n",
        "  return ax\n",
        "\n",
        "def crop_center(\n",
        "  img: torch.Tensor,\n",
        "  frac: float = 0.5\n",
        ") -> torch.Tensor:\n",
        " \n",
        "  h_offset = round(img.shape[0] * (frac / 2))\n",
        "  w_offset = round(img.shape[1] * (frac / 2))\n",
        "  return img[h_offset:-h_offset, w_offset:-w_offset]\n",
        "\n",
        "class EarlyStopping:\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    patience: int = 30,\n",
        "    margin: float = 1e-4\n",
        "  ):\n",
        "    self.best_fitness = 0.0  # In our case PSNR\n",
        "    self.best_iter = 0\n",
        "    self.margin = margin\n",
        "    self.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\n",
        "\n",
        "  def __call__(\n",
        "    self,\n",
        "    iter: int,\n",
        "    fitness: float\n",
        "  ):\n",
        "    \n",
        "    if (fitness - self.best_fitness) > self.margin:\n",
        "      self.best_iter = iter\n",
        "      self.best_fitness = fitness\n",
        "    delta = iter - self.best_iter\n",
        "    stop = delta >= self.patience  # stop training if patience exceeded\n",
        "    return stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXTZ79RxYSXA"
      },
      "outputs": [],
      "source": [
        "def init_models():\n",
        "  \n",
        "  # Encoders\n",
        "  encoder = PositionalEncoder(d_input, n_freqs, log_space=log_space)\n",
        "  encode = lambda x: encoder(x)\n",
        "\n",
        "  # View direction encoders\n",
        "  if use_viewdirs:\n",
        "    encoder_viewdirs = PositionalEncoder(d_input, n_freqs_views,\n",
        "                                        log_space=log_space)\n",
        "    encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
        "    d_viewdirs = encoder_viewdirs.d_output\n",
        "  else:\n",
        "    encode_viewdirs = None\n",
        "    d_viewdirs = None\n",
        "\n",
        "  # Models\n",
        "  model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
        "              d_viewdirs=d_viewdirs)\n",
        "  model.to(device)\n",
        "  model_params = list(model.parameters())\n",
        "  if use_fine_model:\n",
        "    fine_model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
        "                      d_viewdirs=d_viewdirs)\n",
        "    fine_model.to(device)\n",
        "    model_params = model_params + list(fine_model.parameters())\n",
        "  else:\n",
        "    fine_model = None\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.Adam(model_params, lr=lr)\n",
        "\n",
        "  # Early Stopping\n",
        "  warmup_stopper = EarlyStopping(patience=50)\n",
        "\n",
        "  return model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CH6R8jD_Ywp6"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Bucle de entrenamiento en Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r87VAJ0E7aKQ"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "\n",
        "  # Shuffle rays across all images.\n",
        "  if not one_image_per_step:\n",
        "    height, width = images.shape[1:3]\n",
        "    all_rays = torch.stack([torch.stack(get_rays(height, width, focal, p), 0)\n",
        "                        for p in poses[:n_training]], 0)\n",
        "    rays_rgb = torch.cat([all_rays, images[:, None]], 1)\n",
        "    rays_rgb = torch.permute(rays_rgb, [0, 2, 3, 1, 4])\n",
        "    rays_rgb = rays_rgb.reshape([-1, 3, 3])\n",
        "    rays_rgb = rays_rgb.type(torch.float32)\n",
        "    rays_rgb = rays_rgb[torch.randperm(rays_rgb.shape[0])]\n",
        "    i_batch = 0\n",
        "\n",
        "  train_psnrs = []\n",
        "  val_psnrs = []\n",
        "  iternums = []\n",
        "  for i in trange(n_iters):\n",
        "    model.train()\n",
        "\n",
        "    if one_image_per_step:\n",
        "      # Randomly pick an image as the target.\n",
        "      target_img_idx = np.random.randint(images.shape[0])\n",
        "      target_img = images[target_img_idx].to(device)\n",
        "      if center_crop and i < center_crop_iters:\n",
        "        target_img = crop_center(target_img)\n",
        "      height, width = target_img.shape[:2]\n",
        "      target_pose = poses[target_img_idx].to(device)\n",
        "      rays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
        "      rays_o = rays_o.reshape([-1, 3])\n",
        "      rays_d = rays_d.reshape([-1, 3])\n",
        "    else:\n",
        "      # Random over all images.\n",
        "      batch = rays_rgb[i_batch:i_batch + batch_size]\n",
        "      batch = torch.transpose(batch, 0, 1)\n",
        "      rays_o, rays_d, target_img = batch\n",
        "      height, width = target_img.shape[:2]\n",
        "      i_batch += batch_size\n",
        "      # Shuffle after one epoch\n",
        "      if i_batch >= rays_rgb.shape[0]:\n",
        "          rays_rgb = rays_rgb[torch.randperm(rays_rgb.shape[0])]\n",
        "          i_batch = 0\n",
        "    target_img = target_img.reshape([-1, 3])\n",
        "\n",
        "    # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
        "    outputs = nerf_forward(rays_o, rays_d,\n",
        "                           near, far, encode, model,\n",
        "                           kwargs_sample_stratified=kwargs_sample_stratified,\n",
        "                           n_samples_hierarchical=n_samples_hierarchical,\n",
        "                           kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
        "                           fine_model=fine_model,\n",
        "                           viewdirs_encoding_fn=encode_viewdirs,\n",
        "                           chunksize=chunksize)\n",
        "    \n",
        "    # Check for any numerical issues.\n",
        "    for k, v in outputs.items():\n",
        "      if torch.isnan(v).any():\n",
        "        print(f\"! [Numerical Alert] {k} contains NaN.\")\n",
        "      if torch.isinf(v).any():\n",
        "        print(f\"! [Numerical Alert] {k} contains Inf.\")\n",
        "\n",
        "    # Backprop!\n",
        "    rgb_predicted = outputs['rgb_map']\n",
        "    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    psnr = -10. * torch.log10(loss)\n",
        "    train_psnrs.append(psnr.item())\n",
        "\n",
        "    # Evaluate testimg at given display rate.\n",
        "    if i % display_rate == 0:\n",
        "      model.eval()\n",
        "      height, width = testimg.shape[:2]\n",
        "      rays_o, rays_d = get_rays(height, width, focal, testpose)\n",
        "      rays_o = rays_o.reshape([-1, 3])\n",
        "      rays_d = rays_d.reshape([-1, 3])\n",
        "      outputs = nerf_forward(rays_o, rays_d,\n",
        "                             near, far, encode, model,\n",
        "                             kwargs_sample_stratified=kwargs_sample_stratified,\n",
        "                             n_samples_hierarchical=n_samples_hierarchical,\n",
        "                             kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
        "                             fine_model=fine_model,\n",
        "                             viewdirs_encoding_fn=encode_viewdirs,\n",
        "                             chunksize=chunksize)\n",
        "\n",
        "      rgb_predicted = outputs['rgb_map']\n",
        "      loss = torch.nn.functional.mse_loss(rgb_predicted, testimg.reshape(-1, 3))\n",
        "      print(\"Loss:\", loss.item())\n",
        "      val_psnr = -10. * torch.log10(loss)\n",
        "      val_psnrs.append(val_psnr.item())\n",
        "      iternums.append(i)\n",
        "\n",
        "      # Plot example outputs\n",
        "      fig, ax = plt.subplots(1, 4, figsize=(24,4), gridspec_kw={'width_ratios': [1, 1, 1, 3]})\n",
        "      ax[0].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
        "      ax[0].set_title(f'Iteration: {i}')\n",
        "      ax[1].imshow(testimg.detach().cpu().numpy())\n",
        "      ax[1].set_title(f'Target')\n",
        "      ax[2].plot(range(0, i + 1), train_psnrs, 'r')\n",
        "      ax[2].plot(iternums, val_psnrs, 'b')\n",
        "      ax[2].set_title('PSNR (train=red, val=blue')\n",
        "      z_vals_strat = outputs['z_vals_stratified'].view((-1, n_samples))\n",
        "      z_sample_strat = z_vals_strat[z_vals_strat.shape[0] // 2].detach().cpu().numpy()\n",
        "      if 'z_vals_hierarchical' in outputs:\n",
        "        z_vals_hierarch = outputs['z_vals_hierarchical'].view((-1, n_samples_hierarchical))\n",
        "        z_sample_hierarch = z_vals_hierarch[z_vals_hierarch.shape[0] // 2].detach().cpu().numpy()\n",
        "      else:\n",
        "        z_sample_hierarch = None\n",
        "      _ = plot_samples(z_sample_strat, z_sample_hierarch, ax=ax[3])\n",
        "      ax[3].margins(0)\n",
        "      plt.show()\n",
        "\n",
        "    # Check PSNR for issues and stop if any are found.\n",
        "    if i == warmup_iters - 1:\n",
        "      if val_psnr < warmup_min_fitness:\n",
        "        print(f'Val PSNR {val_psnr} below warmup_min_fitness {warmup_min_fitness}. Stopping...')\n",
        "        return False, train_psnrs, val_psnrs\n",
        "    elif i < warmup_iters:\n",
        "      if warmup_stopper is not None and warmup_stopper(i, psnr):\n",
        "        print(f'Train PSNR flatlined at {psnr} for {warmup_stopper.patience} iters. Stopping...')\n",
        "        return False, train_psnrs, val_psnrs\n",
        "    \n",
        "  return True, train_psnrs, val_psnrs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejecución de la sesión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MnHIjU4IYfJp",
        "outputId": "ccceff2f-8783-4419-e670-15402eb95736"
      },
      "outputs": [],
      "source": [
        "# Run training session(s)\n",
        "for _ in range(n_restarts):\n",
        "  model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper = init_models()\n",
        "  success, train_psnrs, val_psnrs = train()\n",
        "  if success and val_psnrs[-1] >= warmup_min_fitness:\n",
        "    print('Training successful!')\n",
        "    break\n",
        "\n",
        "print('')\n",
        "print(f'Done!')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Salvado de los modelos en pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb3TZd06zFP0"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'nerf.pt')\n",
        "torch.save(fine_model.state_dict(), 'nerf-fine.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nerfintro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "8e65dcaae3c5fe2fef3791e6b78a04294d71d23af0bc69d7e72fdf744c50808a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
